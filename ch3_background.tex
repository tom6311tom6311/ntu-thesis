\chapter{Background}
\label{c:background}

In this chapter, we review important properties of several technologies or terms that are used in FileFarm.

% Cloud Computing
\section{Cloud Computing}
\label{s:cloudcomputing}

Cloud computing is a model describing the relationship between computing resources, service providers and consumers over network connections. In this model, service provider makes effort to offer consumers a convenient, reliable and on-demand way of accessing computing resources. The provider's computing resources are normally pooled to serve multiple consumers using a multi-tenant way with different physical and virtual resources dynamically assigned and reassigned according to consumer's demand. From consumer's point of view, the capabilities available often appear to be unlimited and can be elastically provisioned and released in any amount as long as they need. Furthermore, the resource usage can usually be measured with certain metrics (e.g., storage, processing, bandwidth and active user accounts) and be reported to both provider and consumers transparently. Based on a signed contract, provider then charges consumers in regular basis according to resource usage.\cite{mell2011nist}

With cloud computing technology, enterprises obtain an easy way toward computing outsourcing, in a sense that they no longer need to commit a large amount of capital on hardware and software to build their own IT infrastructure before product launch. Instead, they can rapidly allocate just-enough computing power and storage space as they need with minimal management overhead. They can also elastically expand or reduce the amount of allocated resources based on changes of business scale. Due to the flexibility and reliability, cloud computing has given rise to a paradigm shift in how computing services are deployed and delivered.\cite{6123700}

% Cloud Storage Service
\section{Cloud Storage Service}
\label{s:cloudstorageservice}

Among all types of cloud computing services, storage is nearly the most fundamental one that many others are built upon. To offer a reliable, widely-available cloud storage service, providers build infrastructure and data centers spanning all over the world. Besides hardware deployment, providers also develop software to take care of aspects such as access management, resource routing, replication schema, error correction code, load-balancing... All efforts done by providers contribute to a single purpose: to provide consumer with a robust, easy-to-access online disk space. From consumer's point of view, the storage service seems much more simpler. To access the storage service, consumers connect to provider's server using a pre-defined API. The allowed API operations should at least include POST, GET, DELETE, by which consumers can upload, download and remove data, respectively.

Cloud storage service is a mature technology and business that changes the way data are stored and accessed. Among all existing cloud storage services, Amazon AWS S3\cite{awss3}, Google Cloud Storage\cite{googlecloudstorage}, Microsoft Azure\cite{msazurestorage} are 3 successful instances. Depending on usage size, access rate and service-level agreement, a storage provider may provide various storage service products. The pricing schema of cloud storage services usually involves 2 major usage factors: (1) $static$ $storage$ (2) $data$ $transfer$. The former one is the fee of storing data statically, charged in per GB/month basis, whereas the latter one charges consumers every GB of data downloaded from the cloud (upload traffics are usually for free). The 2 factors collectively account for the primary part of storage fee.

Cloud storage services have successfully relieve individual or enterprise consumers from affording the high cost of maintaining their own storage systems. Due to the economies of scale, cost of cloud storage solution is usually much lower than that of building a storage infrastructure with same level of reliability on consumers' own. Thus, cloud storage has gradually dominated the choice of enterprises over other storage options\cite{storagetrends2018}.

\newpage

% Cloud-of-Clouds
\section{Cloud-of-Clouds}
\label{s:cloudofclouds}

Although cloud computing provides benefits in terms of low cost, elasticity and reliability, ensuring security of data stored on clouds remains an unsolved problem, as consumers often store critical and sensitive data on the services offered by a third-party provider which may be untrusted. To solve this issue, researchers have moved forward to  a new research domain\cite{aizain2012multiclouds}. "Cloud-of-clouds", or "multi-clouds", "interclouds" is an area of research aiming to build a service on multiple clouds and avoid dependence or data leakage on any of them. The term was firstly introduced by Vukolic\cite{vukolic2010byzantine}. Multiple works have been proposed at around the same time\cite{bowers2009hail},\cite{abu2010racs},\cite{cachin2010dependable},\cite{bessani2013depsky}. The research in this area has often been formulated as a Byzantine fault tolerance problem\cite{lamport1982byzantine}, in the sense that any faults occurring to a cloud may lead to misbehavior of it, while the system is designed to tolerate certain level of concurrent cloud failures. Besides, approaches of solving the single-cloud problem often integrate certain means of coding techniques that not only add redundancy to data, but also make sure that a file is not view-able or recoverable from any single cloud. This way, risks of malicious insider or single cloud service failure can be reduced significantly.

% Hybrid Cloud
\section{Hybrid Cloud}
\label{s:hybridcloud}

Hybrid cloud is a specialized branch of cloud-of-clouds research. In the settings of hybrid cloud, part of the enterprises' service is held by their own servers. Integration of both public clouds and private servers contributes to a more rapid and robust service. Such hybrid design is actually a more practical and feasible deployment option for most enterprises. As mentioned above, public clouds bring benefits of reliability, flexibility and cost-efficiency; however, they cannot provide certain benefits of servers in private network, such as security, low-latency and full control over data. With proper design, hybrid cloud systems have the potential of bringing the best of both public clouds and private premises, while minimizing the disadvantages of them. A cloud storage example under this topic is: $Hybris$\cite{dobre2014hybris}.

\newpage

% Peer-to-Peer Systems
\section{Peer-to-Peer Systems}
\label{s:peertopeersystems}

Peer-to-Peer is a computing or networking architecture in which each participant, called a $node$ or a $peer$, shares equal responsibility of maintaining the dedicated service. In a P2P system, peers are equally-privileged and follow the same protocol to negotiate with each other, without a centralized coordinator over them. Instead of aggregating storage and computing power to a single serving machine, each peer in a P2P system contributes part of their resources to the network, and requires the resources they want from other peers in return. A properly-designed P2P system will eventually meet all peers' need. Because of the distributed architecture, P2P model does not suffer from single-point-of-failure problem, which means such systems are generally robust to frequent churning of peers, network topology changes, or failures occurring to part of the network. Besides, since peers serve need for each other without a centralized channel, P2P systems tend to have better throughput and service capability than traditional client-server model. However, since there are no central node in a P2P network, a peer needs to coordinate with other peers on it's own and achieve a consistent state of consensus with others, in terms of routing information, content location, status of other peers, ... This brings extra computational and timing overhead to peer applications. Due to this, P2P protocols are usually more difficult to design then client-server ones, and some P2P systems are suffering from efficiency and scalability issues. Despite of the difficulty, P2P systems had still found its popularity in many application domains, with content sharing being the origin of the whole concept and the most successful one so far. $Napster$\cite{napster}, $BitTorrent$\cite{bittorrent}, $eMule$\cite{emule}, $aMule$\cite{amule} are some successful P2P content sharing applications.

\newpage

% Distributed Hash Table
\section{Distributed Hash Table}
\label{s:distributedhashtable}

How to search for data over a fully-distributed network has always been a research topic drawing high attention. Of all kind of means proposed, distributed hash table (DHT) is the most popular category due to its high efficiency. Generally speaking, DHT is a content-addressed approach in which each piece of data is assigned with a $key$ defined as its hash value. The piece of data is then stored in the distributed network according to its $key$. Thus, the locations where a given piece of data should be stored are defined by its content. This introduces deduplication feature for such systems since same content will always be stored on same set of nodes. Besides, the dispersal and randomness properties of hash function also enable DHT-based systems to be designed in a load-balancing way. In P2P applications, DHTs are often implemented as an overlay or infrastructure that more complex services can be built upon. Such applications then inherit the desired features of their underlying DHTs. A DHT protocol also defines how to look up and retrieve data from the network. Efficiency of the look up process has a direct impact on performance and usability of systems based on the DHT. Most of the widely-used DHTs have logarithm-time guarantee on worst-case look up length. Chord\cite{stoica2001chord}, Kademlia\cite{maymounkov2002kademlia}, Pastry\cite{rowstron2001pastry}, Tapestry\cite{zhao2004tapestry} are some well-known DHT protocols, to name a few.


% Kademlia
\section{Kademlia}
\label{s:kademlia}
Kademlia is a popular structured DHT protocol applied on a number of modern P2P applications including BitTorrent\cite{bittorrent}, Ethereum\cite{ethereum}, IPFS\cite{ipfs} and Storij\cite{storij}, ... As described in \ref{s:distributedhashtable}, Kademlia provides an efficient way to look up contents in a P2P network given their hash values. Like many other DHT alternatives, it takes no more than $\lceil log(n) \rceil + c$ iterated queries to look up any target in a Kademlia network\cite{maymounkov2002kademlia}. In addition, Kademlia also provides some desired properties that make it popular.

First of all, Kademlia is structured, which means every node in the network has an unique ID, and the IDs collectively form a specific structure that makes look up operations efficient. In Kademlia, each node is assigned with a randomly-generated 160-bit ID when it starts up. The ID space can be represented as a binary tree (also called $trie$) of height 160, with each ID representing a leaf node in the trie. Given 2 IDs, $x$ and $y$, the distance between them is defined as their bit-wise exclusive or (XOR) interpreted as an integer, $d(x,y)=x \oplus y$. Kademlia use SHA-1 as its hash function, which generates 160-bit hash values. Since hash value of contents, also called $keys$, share exactly the same format as node IDs, the distance between a key and a node ID can also be defined by the XOR metric. The XOR distance metric is $unidirectional$. For any given point $x$ and distance $\Delta>0$, there is exactly one point $y$ such that $d(x,y)=\Delta$. $Unidirectionality$ ensures that look ups for the same key converge along the same path, regardless of originating node. Thus, caching data along the path alleviates hot spots. Besides, XOR is also $symmetric$, which means $\forall x, y: d(x,y)=d(y,x)$. $Symmetry$ allows nodes to receive look up queries from precisely the same distribution of nodes in their routing tables. The 2 important properties of XOR metric also ensures that inter-node configuration messages in Kademlia is minimized.

With distance metric defined, Kademlia further regulates that each piece of data should be stored at the $K$ nodes with ID closest to the content's key, where $K$ is a system-wise redundancy parameter. Given content key, the $K$ nodes with ID closest to it are uniquely defined and can be found by a LOOKUP procedure specified in Kademlia protocol. Thus, for a node to upload a piece of data, it first compute $key$ by SHA-1 hash, then use LOOKUP procedure to locate the $K$ nodes closest to the $key$. After that, it sends STORE message to each of the closest nodes. For the case of downloading data, another procedure called VALUE\_LOOKUP is designed. Similar to LOOKUP, VALUE\_LOOKUP also implements an iterative strategy to get closer to the target $key$. But once the target content is returned by any of the visited nodes, the VLAUE\_LOOKUP process finishes immediately. Both LOOKUP and VALUE\_LOOKUP procedures are guaranteed to finish in logarithm steps, while VALUE\_LOOKUP takes a tighter bond with a factor of $H_K = \sum_{i=1}^{K} 1/i$, since only 1 copy of data is required to be found, and we can find it by reaching any of the $K$ storing nodes\cite{cai2013probabilistic}.

Besides minimized configuration messages and efficient look ups, Kademlia is also robust to time delays of failed nodes, since both LOOKUP and VALUE\_LOOKUP procedures use parallel and asynchronous queries. Last but not the least, Kademlia is highly resistant to DoS attacks, since routing table of each node prefers long-lived nodes over new entrants.

With all of the desired properties listed above, Kademlia has become one of the most popular infrastructure for practical and global-scaled P2P applications.

\newpage

% Information Dispersal Algorithm
\section{Information Dispersal Algorithm}
\label{s:informationdispersalalgorithm}

An Information Dispersal Algorithm (IDA)\cite{rabin1989efficient} is a computation schema of disperse content of a file into smaller chunks and reconstructing the original file based on some of the chunks, where the dispersal and reconstruction process are computationally efficient. To be more precise, a $(p,q)$ IDA schema breaks a file $F$ into $p+q$ chunks of size $\frac{|F|}{p}$, such that $F$ can be reconstructed from any p chunks but not less. Figure \ref{fig:ida} shows an example of dispersing a file $F$ of size $|F|=16$ bytes with a $(4,2)$ schema. In the figure, each byte is represented as an integer from $[0, 255]$. The first step of IDA is to split the original file into $p=4$ chunks, with each chunk being a row of the matrix representing the entire file. After that, a randomly generated matrix $M$ of size $(p+q)\times p$ is being multiplied with the file matrix and the computation yields a matrix of size $(p+q)\times \frac{|F|}{p}$. Each row in the result matrix is a computed chunk that is supposed to be stored in a place different from others. To reconstruct the original file $F$, one needs to collect $p$ computed chunks and multiply the collected chunks with inverse matrix of concatenation of their corresponding rows in $M$. As long as the corresponding rows in $M$ are linearly independent, the original file can be reconstructed based on these $p$ collected chunks.

Due to its potential traits of security, load-balancing and fault tolerance design, Information Dispersal Algorithm is widely-used in distributed file storage or content sharing applications. Considering a distributed network of computers and workstations, files pre-processed with IDA are certainly hard to be reconstructed by nodes other than the uploader, since only the uploading node has the knowledge of dispersion keys and distribution of chunks in the network. In a distributed application, the stored chunks can be downloaded from different nodes in parallel, this boosts the downloading performance of large files.  Also, the $q$ factor in the IDA schema makes redundancy and tolerance in a fault-prone environment. These are reasons why IDA is suitable for distributed systems.

\begin{figure}[hbt]
\centering
  \includegraphics[width=13cm]{figures/ida.png}
  \caption{An example of IDA with schema $(p,q)=(4,2)$}
  \label{fig:ida}
\end{figure}

\newpage

% Public Key Infrastructure
\section{Public Key Infrastructure}
\label{s:publickeyinfrastructure}

Public key infrastructure (PKI) is an infrastructure that enables entities to securely communicate over an insecure public network via cryptographic techniques of public-key encryption, digital signature and certificate-based authentication. From the perspective of cryptography, PKI is a reliable approach to bind public keys with respective identities. The binding process is facilitated by registration and issuance of digital certificates, which can be demonstrated by Figure \ref{fig:pki}: The whole process involves 3 parties: an entity providing service to users, an certificate authority (CA) and a user. To get a certificate, the entity needs to generate a certificate request (CSR) containing its public key first. It then send the CSR to CA. With all means of verification, CA finally trusts the entity. CA then generates a certificate for the entity, which includes CA's digital signature signed with its private key. The certificate is then sent back to the entity and installed on its server machine. From now on, whenever a user sends a connection request to the entity, it responses with its certificate. The user then verifies this certificate by validating the CA's signature on the certificate. If the verification passes, user will establish a secure connection encrypted by the entity's public key.

PKI is a general term involving policies of issuing, managing, and validating digital certificates. Inside this broad term, X.509\cite{rfc4158} is a majorly adopted standard defining the format of public key certificates, which is widely used in the Internet and is the fundamental technique implemented in TLS/SSL, which is the basis of HTTPS.

\begin{figure}[hbt]
\centering
  \includegraphics[width=14cm]{figures/pki.png}
  \caption{Procedure of certificate issuance and certificate-based authentication}
  \label{fig:pki}
\end{figure}